"""
This file is for record the old version model.

Rmk: 
How to use LSTM to train nn-classifier?
How to do masking with NLP?
Is it better to do bucketing with different length? Or take batch_size equal to 1?
    
Ref Link:
Dataset:
  https://www.kaggle.com/c/spooky-author-identification/data

NLP competitions on kaggle before: <- useful!!!
  https://www.kaggle.com/c/spooky-author-identification/discussion/42013

Understanding Convolutional Neural Networks for NLP:
  http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/

Multi-input and multi-output models: 
  https://keras.io/getting-started/functional-api-guide/
  
How to choose # of hidden layers unit: Nh = Ns/(alpha*(Ni+No)), 2<alpha<10 or = sqrt(Ni*No)
  https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw

Text Classification with NLTK and Scikit-Learn:
  https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html
"""


"""
  Ver: 0.0.0
  Date: N/A
  Rmk: Template of Keras Sequential Neural Network Model
"""
def create_lstm_network(num_freq_dim, num_hidden_dim, num_recurrent_units=0):
  model = Sequential()
  model.add(LSTM(num_hidden_dim, input_shape=num_freq_dim, activation='relu', return_sequences=True))
  for cur_unit in range(num_recurrent_units):
    model.add(LSTM(num_hidden_dim, return_sequences=True))
  model.add(LSTM(num_hidden_dim))
  model.add(Dropout(0.2))
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(3, activation='softmax'))
  
  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 
  print(model.summary())
  return model


"""
  Ver: 0.0.1
  Date: 2017/10/30, 23:55
  Score: around 0.6 (forgot to mark down...)
  Rmk: Using LSTM only, But this take too many parameter...
"""
def create_model():
  nn = Sequential()
  nn.add(LSTM(2046, input_shape=(None, 200), 
    batch_input_shape=(200, None, 100), stateful=True, activation='tanh', return_sequences=True))
  nn.add(LSTM(512, activation='relu'))
  nn.add(Dropout(0.2))
  nn.add(Dense(128, activation='relu'))
  nn.add(Dropout(0.2))
  nn.add(Dense(32, activation='relu'))
  nn.add(Dropout(0.2))
  nn.add(Dense(3, activation='softmax'))
  
  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  nn.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 
  print(nn.summary())
  return nn


"""
  Ver: 0.1.2
  Date: 2017/10/31, 19:51
  Score: acc=0.5307, val_acc=0.5625
  Rmk: LSTM with Convolution and Max Pooling, this show maxpooling should be better than avgpooling.
"""
def create_model(var_len=200, num_recurrent_units=0):
  nn = Sequential()
  nn.add(LSTM(256, input_shape=(None, 200), 
    batch_input_shape=(200, None, 200), stateful=True, activation='tanh', return_sequences=True))
  nn.add(LSTM(256, activation='relu', return_sequences=True))
  # convolution1D
  nn.add(Conv1D(64, kernel_size=16, padding='causal', activation='relu'))
  # max-pooling
  nn.add(GlobalMaxPooling1D()) # from 3D to 2D or GlobalAveragePooling1D, since Flatten() is only dealing with fixed input shape
  # Ref: https://github.com/fchollet/keras/issues/1920
  nn.add(Dropout(0.25))
  nn.add(Dense(64, activation='relu'))
  nn.add(Dropout(0.25))
  nn.add(Dense(16, activation='relu'))
  nn.add(Dropout(0.25))
  nn.add(Dense(3, activation='softmax'))
  
  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  nn.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 
  print(nn.summary())
  return nn


"""
  Ver: 0.0.3
  Date: 2017/10/31, 20:28
  Score: acc=0.7728, val_acc=0.4013 (0.5725 at 18ecophs)
  Rmk: Recurrent Convolution Neural Network. Overfitted.
"""
def create_model(var_len=200, num_recurrent_units=2):
  nn = Sequential()
  # Convolution1D
  nn.add(Conv1D(256, input_shape=(None, 200), 
    batch_input_shape=(200, None, 200), kernel_size=32, padding='causal', activation='tanh'))
  for i in range(num_recurrent_units):
    nn.add(Conv1D(int(256/(2**(i+1))), kernel_size=int(32/(2**(i+1))), padding='causal', activation='relu'))
  # nn.add(Conv1D(64, kernel_size=16, padding='causal', activation='relu'))
  # MaxPooling/GlobalAveragePooling
  # from 3D to 2D or GlobalAveragePooling1D, since Flatten() is only dealing with fixed input shape
  # Ref: https://github.com/fchollet/keras/issues/1920
  nn.add(GlobalMaxPooling1D()) 
  nn.add(Dropout(0.25))
  nn.add(Dense(64, activation='relu'))
  nn.add(Dropout(0.25))
  nn.add(Dense(16, activation='relu'))
  nn.add(Dropout(0.25))
  nn.add(Dense(3, activation='softmax'))
  
  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
  nn.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 
  print(nn.summary())
  return nn
